{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/bkkaggle/pytorch-CycleGAN-and-pix2pix/blob/master/CycleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2870,"status":"ok","timestamp":1654865753943,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"},"user_tz":-420},"id":"LoITfd0hsefv","outputId":"882cd10a-662e-48d9-bf4b-54015f1e95c3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QaqlPTJPtdC7","executionInfo":{"status":"ok","timestamp":1654865755481,"user_tz":-420,"elapsed":329,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1654865757010,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"},"user_tz":-420},"id":"BOMHpQWjojRQ","outputId":"ac8af80a-0464-4fc9-b16a-8d8ccd5832d7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import os\n","os.getcwd()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1654865758276,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"},"user_tz":-420},"id":"p1tVClL0ZRAq","outputId":"5b2b4896-81ea-489d-eb61-affa21e21a69"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["600"]},"metadata":{},"execution_count":4}],"source":["len(os.listdir('/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master/datasets/data_v2/testB'))"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2386,"status":"ok","timestamp":1654865762042,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"},"user_tz":-420},"id":"YuGAiqDDsPoN","outputId":"6537c682-801d-4c02-830f-b6dcb01156e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.11.0+cu113)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.12.0+cu113)\n","Requirement already satisfied: dominate>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (2.6.0)\n","Requirement already satisfied: visdom>=0.1.8.8 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.1.8.9)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (0.12.18)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.21.6)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (23.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.4.1)\n","Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.32)\n","Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (0.1.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (5.1.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.3.2)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (1.2.3)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.17.3)\n","Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (1.0.9)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n","Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.27)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (57.4.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (7.1.2)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (0.1.2)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (2.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (5.4.8)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (3.13)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 5)) (1.5.12)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2022.5.18.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->-r requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.3)\n"]}],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"5VIGyIus8Vr7"},"source":["Take a look at the [repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) for more information"]},{"cell_type":"markdown","metadata":{"id":"7wNjDKdQy35h"},"source":["# Install"]},{"cell_type":"markdown","metadata":{"id":"8daqlgVhw29P"},"source":["# Datasets\n","\n","Download one of the official datasets with:\n","\n","-   `bash ./datasets/download_cyclegan_dataset.sh [apple2orange, summer2winter_yosemite, horse2zebra, monet2photo, cezanne2photo, ukiyoe2photo, vangogh2photo, maps, cityscapes, facades, iphone2dslr_flower, ae_photos]`\n","\n","Or use your own dataset by creating the appropriate folders and adding in the images.\n","\n","-   Create a dataset folder under `/dataset` for your dataset.\n","-   Create subfolders `testA`, `testB`, `trainA`, and `trainB` under your dataset's folder. Place any images you want to transform from a to b (cat2dog) in the `testA` folder, images you want to transform from b to a (dog2cat) in the `testB` folder, and do the same for the `trainA` and `trainB` folders."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1654865766188,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"},"user_tz":-420},"id":"vrdOettJxaCc","outputId":"782a9398-0787-4b93-d2f8-e5471c3f4342"},"outputs":[{"output_type":"stream","name":"stdout","text":["./datasets/data_v2: ./datasets/data_v2: Is a directory\n"]}],"source":["!bash ./datasets/data_v2 nightdayimg"]},{"cell_type":"markdown","metadata":{"id":"gdUz4116xhpm"},"source":["# Pretrained models\n","\n","Download one of the official pretrained models with:\n","\n","-   `bash ./scripts/download_cyclegan_model.sh [apple2orange, orange2apple, summer2winter_yosemite, winter2summer_yosemite, horse2zebra, zebra2horse, monet2photo, style_monet, style_cezanne, style_ukiyoe, style_vangogh, sat2map, map2sat, cityscapes_photo2label, cityscapes_label2photo, facades_photo2label, facades_label2photo, iphone2dslr_flower]`\n","\n","Or add your own pretrained model to `./checkpoints/{NAME}_pretrained/latest_net_G.pt`"]},{"cell_type":"markdown","metadata":{"id":"yFw1kDQBx3LN"},"source":["# Training\n","\n","-   `python train.py --dataroot ./datasets/horse2zebra --name horse2zebra --model cycle_gan`\n","\n","Change the `--dataroot` and `--name` to your own dataset's path and model's name. Use `--gpu_ids 0,1,..` to train on multiple GPUs and `--batch_size` to change the batch size. I've found that a batch size of 16 fits onto 4 V100s and can finish training an epoch in ~90s.\n","\n","Once your model has trained, copy over the last checkpoint to a format that the testing model can automatically detect:\n","\n","Use `cp ./checkpoints/horse2zebra/latest_net_G_A.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class A to class B and `cp ./checkpoints/horse2zebra/latest_net_G_B.pth ./checkpoints/horse2zebra/latest_net_G.pth` if you want to transform images from class B to class A.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sp7TCT2x9dB","outputId":"228767cd-3770-495d-e4ba-ca3e7bf3c063","executionInfo":{"status":"ok","timestamp":1654871126053,"user_tz":-420,"elapsed":5080434,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------- Options ---------------\n","               batch_size: 1                             \n","                    beta1: 0.5                           \n","          checkpoints_dir: ./checkpoints                 \n","           continue_train: False                         \n","                crop_size: 512                           \t[default: 256]\n","                 dataroot: ./datasets/data_v2            \t[default: None]\n","             dataset_mode: unaligned                     \n","                direction: AtoB                          \n","              display_env: main                          \n","             display_freq: 400                           \n","               display_id: 1                             \n","            display_ncols: 4                             \n","             display_port: 8097                          \n","           display_server: http://localhost              \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","              epoch_count: 1                             \n","                 gan_mode: lsgan                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: True                          \t[default: None]\n","                 lambda_A: 10.0                          \n","                 lambda_B: 10.0                          \n","          lambda_identity: 0.5                           \n","                load_iter: 0                             \t[default: 0]\n","                load_size: 542                           \t[default: 286]\n","                       lr: 0.0002                        \n","           lr_decay_iters: 50                            \n","                lr_policy: linear                        \n","         max_dataset_size: inf                           \n","                    model: cycle_gan                     \n","                 n_epochs: 20                            \t[default: 100]\n","           n_epochs_decay: 100                           \n","               n_layers_D: 3                             \n","                     name: night2dayV2                   \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: resnet_9blocks                \n","                      ngf: 64                            \n","               no_dropout: True                          \n","                  no_flip: False                         \n","                  no_html: False                         \n","                     norm: instance                      \n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: train                         \n","                pool_size: 50                            \n","               preprocess: resize_and_crop               \n","               print_freq: 100                           \n","             save_by_iter: False                         \n","          save_epoch_freq: 5                             \n","         save_latest_freq: 5000                          \n","           serial_batches: False                         \n","                   suffix:                               \n","         update_html_freq: 1000                          \n","                use_wandb: True                          \t[default: False]\n","                  verbose: False                         \n","----------------- End -------------------\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","dataset [UnalignedDataset] was created\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","The number of training images = 2500\n","initialize network with normal\n","initialize network with normal\n","initialize network with normal\n","initialize network with normal\n","model [CycleGANModel] was created\n","---------- Networks initialized -------------\n","[Network G_A] Total number of parameters : 11.378 M\n","[Network G_B] Total number of parameters : 11.378 M\n","[Network D_A] Total number of parameters : 2.765 M\n","[Network D_B] Total number of parameters : 2.765 M\n","-----------------------------------------------\n","Setting up a new session...\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.18\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master/wandb/run-20220610_130110-302ilazf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnight2dayV2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/strong-ranccoons/CycleGAN-and-pix2pix\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/strong-ranccoons/CycleGAN-and-pix2pix/runs/302ilazf\u001b[0m\n","create web directory ./checkpoints/night2dayV2/web...\n","learning rate 0.0002000 -> 0.0002000\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n","/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","(epoch: 1, iters: 100, time: 1.993, data: 1.509) D_A: 0.279 G_A: 0.253 cycle_A: 3.238 idt_A: 0.631 D_B: 0.329 G_B: 0.317 cycle_B: 1.681 idt_B: 1.494 \n","(epoch: 1, iters: 200, time: 1.990, data: 0.003) D_A: 0.171 G_A: 0.373 cycle_A: 2.183 idt_A: 1.029 D_B: 0.203 G_B: 0.314 cycle_B: 2.289 idt_B: 1.098 \n","(epoch: 1, iters: 300, time: 1.990, data: 0.005) D_A: 0.235 G_A: 0.891 cycle_A: 2.807 idt_A: 0.571 D_B: 0.225 G_B: 0.598 cycle_B: 1.187 idt_B: 1.204 \n","(epoch: 1, iters: 400, time: 10.368, data: 0.003) D_A: 0.112 G_A: 0.469 cycle_A: 1.342 idt_A: 0.419 D_B: 0.242 G_B: 0.372 cycle_B: 0.860 idt_B: 0.720 \n","(epoch: 1, iters: 500, time: 1.990, data: 0.005) D_A: 0.183 G_A: 0.290 cycle_A: 2.025 idt_A: 0.437 D_B: 0.205 G_B: 0.335 cycle_B: 1.158 idt_B: 0.771 \n","(epoch: 1, iters: 600, time: 1.998, data: 0.003) D_A: 0.140 G_A: 0.316 cycle_A: 3.000 idt_A: 0.657 D_B: 0.263 G_B: 0.499 cycle_B: 2.006 idt_B: 1.635 \n","(epoch: 1, iters: 700, time: 1.993, data: 0.003) D_A: 0.187 G_A: 0.279 cycle_A: 1.883 idt_A: 0.572 D_B: 0.246 G_B: 0.418 cycle_B: 1.441 idt_B: 0.931 \n","(epoch: 1, iters: 800, time: 3.476, data: 0.003) D_A: 0.290 G_A: 0.296 cycle_A: 1.599 idt_A: 0.460 D_B: 0.234 G_B: 0.424 cycle_B: 1.226 idt_B: 0.674 \n","(epoch: 1, iters: 900, time: 1.991, data: 0.014) D_A: 0.216 G_A: 0.762 cycle_A: 2.265 idt_A: 1.193 D_B: 0.097 G_B: 0.386 cycle_B: 2.796 idt_B: 0.993 \n","(epoch: 1, iters: 1000, time: 1.996, data: 0.003) D_A: 0.304 G_A: 0.407 cycle_A: 2.566 idt_A: 0.710 D_B: 0.292 G_B: 0.639 cycle_B: 1.436 idt_B: 1.202 \n","(epoch: 1, iters: 1100, time: 1.994, data: 0.004) D_A: 0.221 G_A: 0.458 cycle_A: 1.922 idt_A: 0.733 D_B: 0.140 G_B: 0.359 cycle_B: 1.326 idt_B: 0.784 \n","(epoch: 1, iters: 1200, time: 3.551, data: 0.003) D_A: 0.158 G_A: 0.599 cycle_A: 0.910 idt_A: 0.761 D_B: 0.217 G_B: 0.349 cycle_B: 1.357 idt_B: 0.406 \n","(epoch: 1, iters: 1300, time: 1.992, data: 0.007) D_A: 0.138 G_A: 0.528 cycle_A: 1.659 idt_A: 0.402 D_B: 0.121 G_B: 0.598 cycle_B: 0.979 idt_B: 0.719 \n","(epoch: 1, iters: 1400, time: 1.995, data: 0.003) D_A: 0.226 G_A: 0.857 cycle_A: 2.587 idt_A: 0.416 D_B: 0.186 G_B: 0.607 cycle_B: 1.012 idt_B: 1.006 \n","(epoch: 1, iters: 1500, time: 1.999, data: 0.003) D_A: 0.324 G_A: 0.346 cycle_A: 1.165 idt_A: 0.718 D_B: 0.317 G_B: 0.286 cycle_B: 1.380 idt_B: 0.666 \n","(epoch: 1, iters: 1600, time: 3.629, data: 0.003) D_A: 0.233 G_A: 0.481 cycle_A: 1.448 idt_A: 0.843 D_B: 0.341 G_B: 0.250 cycle_B: 1.539 idt_B: 0.745 \n","(epoch: 1, iters: 1700, time: 1.987, data: 0.017) D_A: 0.187 G_A: 0.403 cycle_A: 1.306 idt_A: 0.679 D_B: 0.117 G_B: 0.917 cycle_B: 1.521 idt_B: 0.546 \n","(epoch: 1, iters: 1800, time: 1.985, data: 0.005) D_A: 0.155 G_A: 0.327 cycle_A: 2.693 idt_A: 0.371 D_B: 0.537 G_B: 0.965 cycle_B: 0.806 idt_B: 1.812 \n","(epoch: 1, iters: 1900, time: 1.994, data: 0.003) D_A: 0.113 G_A: 0.396 cycle_A: 1.436 idt_A: 1.061 D_B: 0.088 G_B: 0.648 cycle_B: 1.949 idt_B: 0.648 \n","(epoch: 1, iters: 2000, time: 4.833, data: 0.003) D_A: 0.069 G_A: 0.589 cycle_A: 1.295 idt_A: 0.268 D_B: 0.186 G_B: 0.460 cycle_B: 1.011 idt_B: 0.648 \n","(epoch: 1, iters: 2100, time: 1.999, data: 0.010) D_A: 0.248 G_A: 0.326 cycle_A: 2.131 idt_A: 0.432 D_B: 0.246 G_B: 0.530 cycle_B: 0.930 idt_B: 0.966 \n","(epoch: 1, iters: 2200, time: 1.994, data: 0.003) D_A: 0.114 G_A: 0.823 cycle_A: 2.256 idt_A: 0.249 D_B: 0.064 G_B: 0.771 cycle_B: 0.572 idt_B: 0.908 \n","(epoch: 1, iters: 2300, time: 1.997, data: 0.003) D_A: 0.060 G_A: 0.495 cycle_A: 2.336 idt_A: 0.350 D_B: 0.047 G_B: 0.800 cycle_B: 1.016 idt_B: 1.176 \n","(epoch: 1, iters: 2400, time: 3.775, data: 0.003) D_A: 0.119 G_A: 0.874 cycle_A: 1.051 idt_A: 0.398 D_B: 0.168 G_B: 1.131 cycle_B: 0.940 idt_B: 0.467 \n","(epoch: 1, iters: 2500, time: 1.994, data: 0.004) D_A: 0.159 G_A: 0.434 cycle_A: 1.411 idt_A: 0.400 D_B: 0.150 G_B: 0.417 cycle_B: 0.723 idt_B: 0.631 \n","End of epoch 1 / 120 \t Time Taken: 4722 sec\n","learning rate 0.0002000 -> 0.0002000\n","(epoch: 2, iters: 100, time: 2.000, data: 1.208) D_A: 0.073 G_A: 0.751 cycle_A: 1.035 idt_A: 0.369 D_B: 0.134 G_B: 0.573 cycle_B: 0.808 idt_B: 0.562 \n","Traceback (most recent call last):\n","  File \"train.py\", line 52, in <module>\n","    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n","  File \"/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master/models/cycle_gan_model.py\", line 187, in optimize_parameters\n","    self.backward_G()             # calculate gradients for G_A and G_B\n","  File \"/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master/models/cycle_gan_model.py\", line 178, in backward_G\n","    self.loss_G.backward()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 363, in backward\n","    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 175, in backward\n","    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","KeyboardInterrupt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:     D_A ▇▄▆▂▄▃▄▇▅▇▅▄▃▅█▆▄▄▂▁▆▂▁▃▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:     D_B ▅▃▄▄▃▄▄▄▂▅▂▃▂▃▅▅▂█▂▃▄▁▁▃▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:     G_A ▁▂█▃▁▂▁▁▇▃▃▅▄█▂▄▃▂▃▅▂▇▄█▃▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:     G_B ▂▂▄▂▂▃▂▂▂▄▂▂▄▄▁▁▆▇▄▃▃▅▅█▂▄\n","\u001b[34m\u001b[1mwandb\u001b[0m: cycle_A █▅▇▂▄▇▄▃▅▆▄▁▃▆▂▃▂▆▃▂▅▅▅▁▃▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: cycle_B ▄▆▃▂▃▆▄▃█▄▃▃▂▂▄▄▄▂▅▂▂▁▂▂▁▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   idt_A ▄▇▃▂▂▄▃▃█▄▅▅▂▂▄▅▄▂▇▁▂▁▂▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:   idt_B ▆▄▅▃▃▇▄▂▄▅▃▁▃▄▂▃▂█▂▂▄▃▅▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:     D_A 0.07285\n","\u001b[34m\u001b[1mwandb\u001b[0m:     D_B 0.13354\n","\u001b[34m\u001b[1mwandb\u001b[0m:     G_A 0.75113\n","\u001b[34m\u001b[1mwandb\u001b[0m:     G_B 0.57343\n","\u001b[34m\u001b[1mwandb\u001b[0m: cycle_A 1.03535\n","\u001b[34m\u001b[1mwandb\u001b[0m: cycle_B 0.80761\n","\u001b[34m\u001b[1mwandb\u001b[0m:   idt_A 0.36933\n","\u001b[34m\u001b[1mwandb\u001b[0m:   idt_B 0.56202\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mnight2dayV2\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/strong-ranccoons/CycleGAN-and-pix2pix/runs/302ilazf\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 49 media file(s), 9 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220610_130110-302ilazf/logs\u001b[0m\n"]}],"source":["!python train.py --dataroot ./datasets/data_v2 --name night2dayV2 --model cycle_gan  --use_wandb  --load_size 542 --crop_size 512"]},{"cell_type":"markdown","metadata":{"id":"9UkcaFZiyASl"},"source":["# Testing\n","\n","-   `python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout`\n","\n","Change the `--dataroot` and `--name` to be consistent with your trained model's configuration.\n","\n","> from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix:\n","> The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.\n","\n","> For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model."]},{"cell_type":"code","source":["cp ./checkpoints/night2dayV2/latest_net_G_A.pth ./checkpoints/night2dayV2/latest_net_G.pth"],"metadata":{"id":"-ZyvjnCF6lhM","executionInfo":{"status":"ok","timestamp":1654865103816,"user_tz":-420,"elapsed":2397,"user":{"displayName":"04-602-ณฐกร คเชนทร","userId":"07359763342963887479"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCsKkEq0yGh0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bec5cbc2-8ea4-485f-908d-6df2e73a7bd8"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------- Options ---------------\n","             aspect_ratio: 1.0                           \n","               batch_size: 1                             \n","          checkpoints_dir: ./checkpoints                 \n","                crop_size: 256                           \n","                 dataroot: ./datasets/data_v2/testB      \t[default: None]\n","             dataset_mode: single                        \n","                direction: AtoB                          \n","          display_winsize: 256                           \n","                    epoch: latest                        \n","                     eval: False                         \n","                  gpu_ids: 0                             \n","                init_gain: 0.02                          \n","                init_type: normal                        \n","                 input_nc: 3                             \n","                  isTrain: False                         \t[default: None]\n","                load_iter: 0                             \t[default: 0]\n","                load_size: 256                           \n","         max_dataset_size: inf                           \n","                    model: test                          \n","             model_suffix:                               \n","               n_layers_D: 3                             \n","                     name: night2dayV2                   \t[default: experiment_name]\n","                      ndf: 64                            \n","                     netD: basic                         \n","                     netG: resnet_9blocks                \n","                      ngf: 64                            \n","               no_dropout: True                          \t[default: False]\n","                  no_flip: False                         \n","                     norm: instance                      \n","                 num_test: 600                           \t[default: 50]\n","              num_threads: 4                             \n","                output_nc: 3                             \n","                    phase: test                          \n","               preprocess: resize_and_crop               \n","              results_dir: ./results/                    \n","           serial_batches: False                         \n","                   suffix:                               \n","                use_wandb: True                          \t[default: False]\n","                  verbose: False                         \n","----------------- End -------------------\n","/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:333: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n","  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n","dataset [SingleDataset] was created\n","initialize network with normal\n","model [TestModel] was created\n","loading the model from ./checkpoints/night2dayV2/latest_net_G.pth\n","---------- Networks initialized -------------\n","[Network G] Total number of parameters : 11.378 M\n","-----------------------------------------------\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: "]}],"source":["!python test.py --dataroot ./datasets/data_v2/testB --name night2dayV2 --model test --no_dropout --use_wandb --num_test 600"]},{"cell_type":"markdown","metadata":{"id":"OzSKIPUByfiN"},"source":["# Visualize"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Mgg8raPyizq"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_fake.png')\n","plt.imshow(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0G3oVH9DyqLQ"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","img = plt.imread('./results/horse2zebra_pretrained/test_latest/images/n02381460_1010_real.png')\n","plt.imshow(img)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"512 CycleGANV2.ipynb","provenance":[]},"environment":{"name":"tf2-gpu.2-3.m74","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m74"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":0}