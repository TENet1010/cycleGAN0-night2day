create web directory ./checkpoints/night2day/web...
learning rate 0.0002000 -> 0.0002000
/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  cpuset_checked))
(epoch: 1, iters: 100, time: 0.541, data: 0.418) D_A: 0.173 G_A: 0.429 cycle_A: 2.962 idt_A: 0.592 D_B: 0.424 G_B: 0.426 cycle_B: 1.136 idt_B: 1.393
(epoch: 1, iters: 200, time: 0.583, data: 0.002) D_A: 0.171 G_A: 0.435 cycle_A: 2.154 idt_A: 0.349 D_B: 0.466 G_B: 1.467 cycle_B: 0.850 idt_B: 0.913
(epoch: 1, iters: 300, time: 0.569, data: 0.002) D_A: 0.148 G_A: 0.829 cycle_A: 2.315 idt_A: 0.387 D_B: 0.174 G_B: 0.321 cycle_B: 0.830 idt_B: 1.005
(epoch: 1, iters: 400, time: 4.455, data: 0.002) D_A: 0.139 G_A: 0.473 cycle_A: 2.551 idt_A: 0.601 D_B: 0.185 G_B: 0.621 cycle_B: 1.182 idt_B: 1.283
Traceback (most recent call last):
  File "train.py", line 52, in <module>
    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights
  File "/content/drive/MyDrive/cycleGAN/pytorch-CycleGAN-and-pix2pix-master/models/cycle_gan_model.py", line 188, in optimize_parameters
    self.optimizer_G.step()       # update G_A and G_B's weights
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py", line 153, in step
    maximize=group['maximize'])
  File "/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py", line 105, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt